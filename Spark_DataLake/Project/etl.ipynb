{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "#config.read('dl.cfg')\n",
    "\n",
    "#os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']\n",
    "#os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_spark_session():\n",
    "    print(\"Creating Spark Session\")\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Created Spark Session\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_song_data(spark, input_data, output_data):\n",
    "    \n",
    "    # get filepath to song data file\n",
    "    #song_data = input_data + \"song-data/*/*/*/*.json\"\n",
    "    song_data = input_data\n",
    "    print(song_data)\n",
    "    \n",
    "    # read song data file\n",
    "    print(\"Reading song_data files from {}...\".format(song_data))\n",
    "    #df = spark.read.json(song_data, schema=songdata_schema)\n",
    "    df = spark.read.format(\"json\").load(song_data)\n",
    "    df.printSchema()\n",
    "    print(\"Finished reading song_data files from {}...\".format(song_data))\n",
    "    \n",
    "\n",
    "    # extract columns to create songs table\n",
    "    print(\"Extracting columns to create song_table ... \")\n",
    "    songs_table = df.select('song_id', 'artist_id', 'year', 'duration')\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    print(\"Writing to parquet files partitioned by year & artist ... \")\n",
    "    songs_table.write.mode('overwrite').mode('overwrite').partitionBy('year', 'artist_id').parquet(output_data + \"songs\")\n",
    "    print(\"Done\")\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    print(\"Extracting columns to create artists_table ... \")\n",
    "    artists_table = df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude','artist_longitude')\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    print(\"Writing to parquet files ... \")\n",
    "    artists_table.write.mode('overwrite').mode('overwrite').parquet(output_data + \"artists\")\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_log_data(spark, input_data, output_data): \n",
    "  \n",
    "    # get filepath to log data file\n",
    "    log_data = input_data\n",
    "\n",
    "    # read log data file\n",
    "    print(\"Reading Log Data files ... \")\n",
    "    #df = spark.read.json(log_data, schema = logdata_schema)\n",
    "    df = spark.read.json(log_data)\n",
    "    df.printSchema()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(col(\"page\") == 'NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    print(\"Extracting Columns for User Table ... \")\n",
    "    users_table = df.select(\"userId\",\"firstName\", \"lastname\",\"gender\",\"level\") \n",
    "    print(\"Done\")\n",
    "\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    print(\"Writing Users table to parquet files\")\n",
    "    users_table.write.mode('overwrite').parquet(output_data+\"users\")\n",
    "    print(\"Done\")\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    print(\"Create timestamp column\")\n",
    "    get_timestamp = udf(lambda x: datetime.fromtimestamp(x / 1000), TimestampType())\n",
    "    df = df.withColumn('timestamp', get_timestamp(df.ts))\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    print(\"Create datetime columns\")\n",
    "    get_datetime = udf(lambda x: F.to_date(x), TimestampType())\n",
    "    df = df.withColumn('datetime', get_datetime(df.ts))\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    print(\"Create time_table\")\n",
    "    \n",
    "    time_table = df.select(col(\"timestamp\").alias(\"start_time\"),\n",
    "                                   hour(col(\"timestamp\")).alias(\"hour\"),\n",
    "                                   dayofmonth(col(\"timestamp\")).alias(\"day\"), \n",
    "                                   weekofyear(col(\"timestamp\")).alias(\"week\"), \n",
    "                                   month(col(\"timestamp\")).alias(\"month\"),\n",
    "                                   year(col(\"timestamp\")).alias(\"year\"))\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    print(\"Time table to Parquet Files\")\n",
    "    #time_table.write.mode('overwrite').partitionBy(\"year\",\"month\").parquet(output_data+\"time\")\n",
    "    time_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(output_data + \"time_table\")\n",
    "    print(\"Done\")\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    print(\"Read Song Data\")\n",
    "    song_data = config['LOCAL']['INPUT_DATA_SD_LOCAL']\n",
    "    song_df = spark.read.json(song_data)\n",
    "    print(\"Done\")\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    #create temp tables\n",
    "    print(\"Creating temp tables\")\n",
    "    log_df = df\n",
    "    log_df.createOrReplaceTempView(\"log_df_table\")\n",
    "    song_df.createOrReplaceTempView(\"song_df_table\")\n",
    "    time_table.createOrReplaceTempView(\"time_table\")\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"Create Songplays Table\")\n",
    "    songplays_table = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT log_df_table.timestamp, \n",
    "            log_df_table.userId, \n",
    "            log_df_table.level, \n",
    "            log_df_table.sessionId, \n",
    "            log_df_table.location,\n",
    "            log_df_table.userAgent, \n",
    "            song_df_table.song_id, \n",
    "            song_df_table.artist_id,\n",
    "            time_table.month,\n",
    "            time_table.year\n",
    "        FROM log_df_table \n",
    "        INNER JOIN song_df_table \n",
    "        ON song_df_table.artist_name = log_df_table.artist \n",
    "        INNER JOIN time_table\n",
    "        ON time_table.start_time = log_df_table.timestamp\n",
    "        \"\"\")\n",
    "    #        GROUP BY time_table.year, time_table.month\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    print(\"Create Songplays parquet files\")\n",
    "    songplays_table.write.mode('overwrite').partitionBy(\"year\",\"month\").parquet(output_data+\"songplays\")\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark Session\n",
      "Created Spark Session\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/song_data/*/*/*/*.json\n",
      "data/output_data/\n"
     ]
    }
   ],
   "source": [
    "#input_data = \"s3a://udacity-dend/\"\n",
    "#output_data = \"s3a://udacity-dend-dalameyer/\"\n",
    "\n",
    "#input_data = config['AWS']['INPUT_DATA_SD']\n",
    "#output_data = config['AWS']['OUTPUT_DATA']\n",
    "input_data = config['LOCAL']['INPUT_DATA_SD_LOCAL']\n",
    "output_data = config['LOCAL']['OUTPUT_DATA_LOCAL']\n",
    "print(input_data)    \n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/song_data/*/*/*/*.json\n",
      "Reading song_data files from data/song_data/*/*/*/*.json...\n",
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "... Finished reading song_data files from data/song_data/*/*/*/*.json...\n",
      "Extracting columns to create song_table ... \n",
      "Done\n",
      "Writing to parquet files partitioned by year & artist ... \n",
      "Done\n",
      "Extracting columns to create artists_table ... \n",
      "Done\n",
      "Writing to parquet files ... \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "process_song_data(spark, input_data, output_data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/log_data/*.json\n",
      "data/output_data/\n"
     ]
    }
   ],
   "source": [
    "input_data = config['LOCAL']['INPUT_DATA_LD_LOCAL']\n",
    "output_data = config['LOCAL']['OUTPUT_DATA_LOCAL']\n",
    "print(input_data)    \n",
    "print(output_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Log Data files ... \n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "Done\n",
      "Extracting Columns for User Table ... \n",
      "Done\n",
      "Writing Users table to parquet files\n",
      "Done\n",
      "Create timestamp column\n",
      "Done\n",
      "Create datetime columns\n",
      "Done\n",
      "Create time_table\n",
      "Done\n",
      "Time table to Parquet Files\n",
      "Done\n",
      "Read Song Data\n",
      "Done\n",
      "Creating temp tables\n",
      "Done\n",
      "Create Songplays Table\n",
      "Done\n",
      "Create Songplays parquet files\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
