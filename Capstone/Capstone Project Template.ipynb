{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project  prepares the immigration, demographic, airpot and temperature data so that the resulting tables are ready to be loaded into a OLAP tabular model such as Microsoft SQL Server Analysis Services or Microsoft PowerBI. The immigration table will be the fact table with records of individuals immigrating to the United sStates. While the remaining tables will be dimension tables that help expand the other atttributes of each record in the fact table. These dimension tables include Demographics which will include data regarding the demographics of US Cities, Temperatures will hold data pertaining to the temperatures of OS Cities, and Airports will contain US Airport information.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "1. **U.S. City Demographic Data:** comes from [OpenSoft](https://public.opendatasoft.com) and includes data by city, state, age, population, veteran status and race.\n",
    "  \n",
    "2. **I94 Immigration Data:** comes from the [US National Tourism and Trade Office](https://travel.trade.gov/research/reports/i94/historical/2016.html) and includes details on incoming immigrants and their ports of entry\n",
    "\n",
    "3. **Airport Code Table:** comes from [datahub.io](https://datahub.io/core/airport-codes#data) and includes airport codes and corresponding cities.\n",
    "\n",
    "4. **World Temperature Data:** comes from [kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) and includes data on temperature changes in the U.S. since 1850.\"\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Configurations and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports and installs\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import configparser\n",
    "import re\n",
    "from pyspark.sql.functions import *\n",
    "from collections import  defaultdict\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'INVALID: STATELESS'),\n",
       " (54, 'No Country Code (54)'),\n",
       " (100, 'No Country Code (100)'),\n",
       " (101, 'ALBANIA'),\n",
       " (102, 'ANDORRA')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Harutaka K. on Student Hub Thread. https://hub.udacity.com/rooms/community:nd027:843753-project-577/community:thread-u22525051-879375?contextType=room\n",
    "def parse_countries(text):\n",
    "  contries_text = re.search(r'I94CIT.+?;', text, re.DOTALL).group(0)\n",
    "  matches = re.findall(r'(.+)=(.+)', contries_text)\n",
    "  matches.sort(key=lambda m: int(m[0]))\n",
    "  countries = {}\n",
    "\n",
    "  for (cid, cname) in matches:\n",
    "    countries[int(cid.strip())] = cname.strip()[1:-1]\n",
    "  return countries\n",
    "\n",
    "with open('I94_SAS_Labels_Descriptions.SAS', 'r') as f:\n",
    "  text = f.read()\n",
    "\n",
    "countries = parse_countries(text)\n",
    "list(countries.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read airport code data:\n",
    "airport_codes_df = pd.read_csv('data/airport-codes.csv', header=0, sep=',')\n",
    "\n",
    "# Read airport code data:\n",
    "demographic_df = pd.read_csv('data/us-cities-demographics.csv', header=0, sep=';')\n",
    "\n",
    "# Read Global country codes data:\n",
    "country_codes_df = pd.DataFrame(list(countries.items()), columns =['Country_Code', 'Country'])\n",
    "\n",
    "# Read City Temperature Data\n",
    "temperature_df =  pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv', header=0, sep=',')\n",
    "\n",
    "#Read Immigration Data\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Read in the data here\n",
    "#Postal Codes: https://fam.state.gov/fam/09FAM/09FAM010205.html\n",
    "postal_codes_df = pd.read_csv('data/PostCodes.csv', header=0, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_Code</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>INVALID: STATELESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>No Country Code (54)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>No Country Code (100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country_Code                Country\n",
       "0             0     INVALID: STATELESS\n",
       "1            54   No Country Code (54)\n",
       "2           100  No Country Code (100)\n",
       "3           101                ALBANIA\n",
       "4           102                ANDORRA"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>City</th>\n",
       "      <th>State_Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>WA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABG</td>\n",
       "      <td>Alburg</td>\n",
       "      <td>VT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>NM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABS</td>\n",
       "      <td>AlburgSprings</td>\n",
       "      <td>VT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADT</td>\n",
       "      <td>AmistadDam</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Code           City State_Code  Region  Country\n",
       "0  ABE       Aberdeen         WA     NaN      NaN\n",
       "1  ABG         Alburg         VT     NaN      NaN\n",
       "2  ABQ    Albuquerque         NM     NaN      NaN\n",
       "3  ABS  AlburgSprings         VT     NaN      NaN\n",
       "4  ADT     AmistadDam         TX     NaN      NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Dictionary from the Postal Codes to look up later\n",
    "code_dict = defaultdict(list)\n",
    "\n",
    "for code,city,state in zip(postal_codes_df.Code, postal_codes_df.City, postal_codes_df.State_Code):\n",
    "    code_dict[code] = [city, state]\n",
    "\n",
    "postal_codes_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "#Build SQL context object\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Import Data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_spark_df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "airport_spark_df=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n",
    "temperature_spark_df=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "country_code_spark_df = spark.createDataFrame(country_codes_df)\n",
    "immigration_spark_df=spark.read.format('com.github.saurfang.sas.spark').load(\"../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "<!--\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Data by City, Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temperature Data\n",
    "\n",
    "Temperature_Clean=temperature_spark_df\\\n",
    "    .filter(temperature_spark_df[\"country\"]==\"United States\")\\\n",
    "    .filter(year(temperature_spark_df[\"dt\"])==2013)\\\n",
    "    .withColumn(\"year\",year(temperature_spark_df[\"dt\"]))\\\n",
    "    .withColumn(\"month\",month(temperature_spark_df[\"dt\"]))\\\n",
    "    .withColumn(\"avg_temp_fahrenheit\",temperature_spark_df[\"AverageTemperature\"]*9/5+32)\\\n",
    "\n",
    "Temperatures=Temperature_Clean.select(\"year\",\"month\",\"Country\", \"City\", round(col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),round(col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\")).dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'month', 'Country', 'City', 'avg_temp_celcius', 'avg_temp_fahrenheit']\n"
     ]
    }
   ],
   "source": [
    "#print(Temperature_df.columns)\n",
    "print(Temperatures.columns)\n",
    "#print(Temperature_Clean.show(1))\n",
    "\n",
    "# Might need City , State Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Data by City, State, Month, Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Immigration_Clean=immigration_spark_df\\\n",
    "    .filter(immigration_spark_df.i94addr.isNotNull())\\\n",
    "    .filter(immigration_spark_df.i94res.isNotNull())\\\n",
    "    .withColumn(\"i94yr\",col(\"i94yr\").cast(\"integer\"))\\\n",
    "    .withColumn(\"i94mon\",col(\"i94mon\").cast(\"integer\")\\\n",
    "    )\n",
    "\n",
    "# Select specifc columns\n",
    "Immigration=Immigration_Clean.select(\\\n",
    "        \"cicid\",col(\"i94yr\").alias(\"year\")\\\n",
    "        ,col(\"i94mon\").alias(\"month\")\\\n",
    "        ,\"i94port\"\\\n",
    "        ,col(\"i94addr\").alias(\"state\")\\\n",
    "        , \"biryear\"\\\n",
    "        , \"gender\"\\\n",
    "        , \"airline\"\\\n",
    "        , \"visatype\"\\\n",
    "        )\n",
    "        #,'visapost', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline', 'admnum', 'fltno', 'visatype'\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+-------+-----+-------+------+-------+--------+\n",
      "|cicid|year|month|i94port|state|biryear|gender|airline|visatype|\n",
      "+-----+----+-----+-------+-----+-------+------+-------+--------+\n",
      "| 41.0|2016|    6|    SFR|   CA| 1994.0|     F|     KE|      F1|\n",
      "+-----+----+-----+-------+-----+-------+------+-------+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "['cicid', 'year', 'month', 'i94port', 'state', 'biryear', 'gender', 'airline', 'visatype']\n"
     ]
    }
   ],
   "source": [
    "#print(Immigration_Clean.columns)\n",
    "#print(Immigration_Clean.head(1))\n",
    "#print(\"--------------\")\n",
    "print(Immigration.show(1))\n",
    "print(Immigration.columns)\n",
    "# Might need City , State Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic Data by City, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Calculate percentages of each numeric column and create new columns.\n",
    "Demographics_Clean=demographics_spark_df\\\n",
    "    .withColumn(\"Median_Age\",col(\"Median Age\").cast(\"float\"))\\\n",
    "    .withColumn(\"%_Male\",demographics_spark_df[\"Male Population\"]/demographics_spark_df[\"Total Population\"]*100)\\\n",
    "    .withColumn(\"%_Female\",demographics_spark_df[\"Female Population\"]/demographics_spark_df[\"Total Population\"]*100)\\\n",
    "    .withColumn(\"%_Veterans\",demographics_spark_df[\"Number of Veterans\"]/demographics_spark_df[\"Total Population\"]*100)\\\n",
    "    .withColumn(\"%_Foreign_Born\",demographics_spark_df[\"Foreign-born\"]/demographics_spark_df[\"Total Population\"]*100)\\\n",
    "    .withColumn(\"%_Race\",demographics_spark_df[\"Count\"]/demographics_spark_df[\"Total Population\"]*100)\\\n",
    "    .orderBy(\"State\")\n",
    "\n",
    "#    .withColumn(\"pct_race\",demog[\"Count\"]/demog[\"Total Population\"]*100)\\\n",
    "\n",
    "#Select desiredd columns and new calculated percentages \n",
    "Demographics_Clean=Demographics_Clean.select(\"City\",\"State\",\"State Code\",\"Median_Age\", \"%_Male\", \"%_Female\", \"%_Veterans\", \"%_Foreign_Born\", \"Race\", \"%_Race\")\n",
    "\n",
    "\n",
    "#pivot the Race column\n",
    "Demographics=Demographics_Clean.groupBy(\"City\",\"State\", \"State Code\",\"Median_Age\", \"%_Male\", \"%_Female\", \"%_Veterans\", \"%_Foreign_Born\").pivot(\"Race\").avg(\"%_Race\")\n",
    "Demographics=Demographics.select(\"City\",\"State\",col(\"State Code\").alias(\"State_Code\"),\"Median_Age\", \"%_Male\", \"%_Female\", \"%_Veterans\", \"%_Foreign_Born\"\\\n",
    "                                       ,col(\"American Indian and Alaska Native\").alias(\"Native_American\")\\\n",
    "                                       , \"Asian\"\\\n",
    "                                       , col(\"Black or African-American\").alias(\"Black_or_African-American\")\\\n",
    "                                       , col(\"Hispanic or Latino\").alias(\"Hispanic_or_Latino\")\\\n",
    "                                       , \"White\"\\\n",
    "                                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+----------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+-------------------------+------------------+------------------+\n",
      "|      City|  State|State_Code|Median_Age|           %_Male|         %_Female|       %_Veterans|    %_Foreign_Born|   Native_American|            Asian|Black_or_African-American|Hispanic_or_Latino|             White|\n",
      "+----------+-------+----------+----------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+-------------------------+------------------+------------------+\n",
      "|    Hoover|Alabama|        AL|      38.5| 44.8378693761124|55.16213062388759| 5.68017067622202| 9.699548556677943|              null|5.609448484777048|       21.441789742924833| 4.042951944270913| 72.92518770848314|\n",
      "|Montgomery|Alabama|        AL|      35.4|47.15284217243477|52.84715782756524|7.455654931052018|4.6548612565184015|0.6366346604448964|3.249479026452494|       60.502727009861104|3.3142891328407766|36.665071340970954|\n",
      "+----------+-------+----------+----------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+-------------------------+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['City',\n",
       " 'State',\n",
       " 'State_Code',\n",
       " 'Median_Age',\n",
       " '%_Male',\n",
       " '%_Female',\n",
       " '%_Veterans',\n",
       " '%_Foreign_Born',\n",
       " 'Native_American',\n",
       " 'Asian',\n",
       " 'Black_or_African-American',\n",
       " 'Hispanic_or_Latino',\n",
       " 'White']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(Demographics.columns)\n",
    "#print(Demographics_Clean.columns)\n",
    "#print(demographics_spark_df.head(1))\n",
    "print(Demographics.show(2))\n",
    "Demographics.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Airports by City, State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter airport data for 'small_airport' in the U.S. and use substring to show state\n",
    "Airports_Clean=airport_spark_df\\\n",
    "    .filter(airport_spark_df[\"iso_country\"]==\"US\")\\\n",
    "    .withColumn(\"iso_region\",substring(airport_spark_df[\"iso_region\"],4,2))\\\n",
    "    .withColumn(\"elevation_ft\",col(\"elevation_ft\").cast(\"float\"))\n",
    "\n",
    "#Select relevant columns and drop duplicates\n",
    "Airports=Airports_Clean.select(col(\"iso_country\").alias(\"Country\"),col(\"iso_region\").alias(\"State_Code\"),col(\"municipality\").alias(\"City\"),\"elevation_ft\", \"type\", \"name\", \"ident\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country', 'State_Code', 'City', 'elevation_ft', 'type', 'name', 'ident']\n",
      "+-------+----------+------------+------------+-------------+--------------------+-----+\n",
      "|Country|State_Code|        City|elevation_ft|         type|                name|ident|\n",
      "+-------+----------+------------+------------+-------------+--------------------+-----+\n",
      "|     US|        PA|    Bensalem|        11.0|     heliport|   Total Rf Heliport|  00A|\n",
      "|     US|        KS|       Leoti|      3435.0|small_airport|Aero B Ranch Airport| 00AA|\n",
      "|     US|        AK|Anchor Point|       450.0|small_airport|        Lowell Field| 00AK|\n",
      "|     US|        AL|     Harvest|       820.0|small_airport|        Epps Airpark| 00AL|\n",
      "|     US|        AR|     Newport|       237.0|       closed|Newport Hospital ...| 00AR|\n",
      "+-------+----------+------------+------------+-------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#print(Demographics.columns)\n",
    "#print(Airports_Clean.columns)\n",
    "print(Airports.columns)\n",
    "#print(Airports.head(5))\n",
    "print(Airports.show(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The __Immigration__ table will be the fact table with records of individuals immigrating to the United sStates. While the remaining tables will be dimension tables that help expand the other atttributes of each record in the fact table. These dimension tables include __Demographics__ which will include data regarding the demographics of US Cities, __Temperatures__ will hold data pertaining to the temperatures of OS Cities, and __Airports__ will contain US Airport information. The key to relate data from the fact table to the dimension tables will be based on the State and City fields which are present on all tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star Schema\n",
    "#### Dimension Tables\n",
    "- __Airport Table__\n",
    "    - Columns: 'Country', 'State_Code', 'City', 'elevation_ft', 'type', 'name', 'ident'\n",
    "    \n",
    "- __Demographic Table__\n",
    "    - Columns: 'City',  'State', 'State_Code', 'Median_Age', '%_Male', '%_Female', '%_Veterans', '%_Foreign_Born', 'Native_American', 'Asian', 'Black_or_African-American', 'Hispanic_or_Latino', 'White'\n",
    "    \n",
    "- __Temperature Table__\n",
    "    - Columns: 'year', 'month', 'Country', 'City', 'avg_temp_celcius', 'avg_temp_fahrenheit'\n",
    "    \n",
    "#### Fact Table:\n",
    "- __Immigration Table__\n",
    "    - Columns: 'cicid', 'year', 'month', 'i94port', 'state', 'biryear', 'gender', 'airline', 'visatype'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stage and Read the data into Spark Data frames\n",
    "2. Implement all required data cleaning and transformation steps\n",
    "3. Merge and/or Join data from different data sources to fill in any data ga  \n",
    "4. Save dataframes into parquet files\n",
    "5. Make files accessible to read for customer use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables in spark session\n",
    "Immigration.createOrReplaceTempView(\"Immigration\")\n",
    "Demographics.createOrReplaceTempView(\"Demographic\")\n",
    "Airports.createOrReplaceTempView(\"Airport\")\n",
    "Temperatures.createOrReplaceTempView(\"Temperature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: /parquet/immigration/immigration_table.parquet_2019_09_26\n",
      "Writing DONE.\n",
      "OUTPUT: /parquet/demographic/demographic_table.parquet_2019_09_26\n",
      "Writing DONE.\n",
      "OUTPUT: /parquet/airport/airport_table.parquet_2019_09_26\n",
      "Writing DONE.\n",
      "OUTPUT: /parquet/temperture/temperture_table.parquet_2019_09_26\n",
      "Writing DONE.\n"
     ]
    }
   ],
   "source": [
    "# Write fact table to parquet\n",
    "output_data = \"/parquet\"\n",
    "start_time = \"2019_09_26\"\n",
    "\n",
    "# Write immigration_table to parquet file:\n",
    "immigration_table_path = output_data + \"/immigration/immigration_table.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {immigration_table_path}\")\n",
    "Immigration.write.mode(\"overwrite\").parquet(immigration_table_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Write demographic to parquet file:\n",
    "demographic_table_path = output_data + \"/demographic/demographic_table.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {demographic_table_path}\")\n",
    "Demographics.write.mode(\"overwrite\").parquet(demographic_table_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Write airport to parquet file:\n",
    "airport_table_path = output_data + \"/airport/airport_table.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {airport_table_path}\")\n",
    "Airports.write.mode(\"overwrite\").parquet(airport_table_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "# Write Temperatures to parquet file:\n",
    "temperture_table_path = output_data + \"/temperture/temperture_table.parquet\" + \"_\" + start_time\n",
    "print(f\"OUTPUT: {temperture_table_path}\")\n",
    "Temperatures.write.mode(\"overwrite\").parquet(temperture_table_path)\n",
    "print(\"Writing DONE.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet file back to Spark:\n",
    "\n",
    "Immigration = spark.read.parquet(immigration_table_path)\n",
    "#Demographics = spark.read.parquet(demographic_table_path)\n",
    "#Airports= spark.read.parquet(airport_table_path)\n",
    "#Temperatures = spark.read.parquet(temperture_table_path)\n",
    "\n",
    "\n",
    "# create a function to be applied in in a spark data frame\n",
    "def get_city(x):\n",
    "    if len(code_dict[x]) == 0:\n",
    "        value = None\n",
    "    else:\n",
    "        value = code_dict[x][0]\n",
    "    return value\n",
    "\n",
    "city_udf_dict = udf(lambda z: get_city(z), StringType())\n",
    "\n",
    "#Add City name based on port name from Immigration dataframe\n",
    "Immigration=Immigration.select(\"cicid\"\\\n",
    "    , \"year\"\\\n",
    "    , \"month\"\\\n",
    "    ,\"i94port\"\\\n",
    "    , city_udf_dict('i94port').alias('City')\\\n",
    "    , \"state\"\\\n",
    "    , \"biryear\"\\\n",
    "    , \"gender\"\\\n",
    "    , \"airline\"\\\n",
    "    , \"visatype\"\\\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration Table row count:  3388925\n",
      "Demographics Table row count:  596\n",
      "Temperatures Table row count:  2313\n",
      "Airports Table row count:  22757\n"
     ]
    }
   ],
   "source": [
    "# Get a Row COunt for each table. ALl tables should have record counts > 0\n",
    "print(\"Immigration Table row count: \", Immigration.count())\n",
    "print(\"Demographics Table row count: \", Demographics.count())\n",
    "print(\"Temperatures Table row count: \", Temperatures.count())\n",
    "print(\"Airports Table row count: \", Airports.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-------+-----+-----+-------+------+-------+--------+\n",
      "|cicid| year|month|i94port| City|state|biryear|gender|airline|visatype|\n",
      "+-----+-----+-----+-------+-----+-----+-------+------+-------+--------+\n",
      "|false|false|false|  false|false|false|  false|  true|   true|   false|\n",
      "|false|false|false|  false| true|false|  false| false|  false|   false|\n",
      "|false|false|false|  false|false|false|  false| false|  false|   false|\n",
      "|false|false|false|  false| true|false|   true| false|   true|   false|\n",
      "|false|false|false|  false|false|false|  false|  true|  false|   false|\n",
      "|false|false|false|  false|false|false|   true| false|   true|   false|\n",
      "|false|false|false|  false| true|false|  false| false|   true|   false|\n",
      "|false|false|false|  false| true|false|  false|  true|   true|   false|\n",
      "|false|false|false|  false|false|false|   true| false|  false|   false|\n",
      "|false|false|false|  false| true|false|   true| false|  false|   false|\n",
      "|false|false|false|  false|false|false|  false| false|   true|   false|\n",
      "|false|false|false|  false| true|false|  false|  true|  false|   false|\n",
      "+-----+-----+-----+-------+-----+-----+-------+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHeck for null values\n",
    "#Cicid requires there be no null values. Any null records for cicid will have to be removed or rechecked.\n",
    "Immigration.select(isnull('cicid').alias('cicid')\\\n",
    "                    , isnull('year').alias('year')\\\n",
    "                    , isnull('month').alias('month')\\\n",
    "                    , isnull('i94port').alias('i94port')\\\n",
    "                    , isnull('City').alias('City')\\\n",
    "                    , isnull('state').alias('state')\\\n",
    "                   , isnull('biryear').alias('biryear')\\\n",
    "                   , isnull('gender').alias('gender')\\\n",
    "                   , isnull('airline').alias('airline')\\\n",
    "                   , isnull('visatype').alias('visatype')\\\n",
    "                  ).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "* Data Dictionary has been added to Data_Dictionary.md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool and Technology Choices:\n",
    "\n",
    "For this project, Apache Spark was utilized to read, transform, and store data. The i94 immigration data contained over 3 million recordds of migrant data into the United States.\n",
    "Utilizing distrbuted technologies for data manipulation like Spark is ideal for working with large data sets.\n",
    "\n",
    "Python was utilized to interact with spark for easier developer experience. And Python's rich community for module development provides many easy options for data manipulations such as pandas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Refresh Schedule\n",
    "\n",
    "A monthly data refresh is recommended for this data model. Many of the data sources such as Immigration and Temperature have the granularity to break down the data by Month and Year of each record.\n",
    "These data sources can be exected to be update from the source on a monthly cadence and the pipeline should match to guarantee frequent data updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approaching New Data problems\n",
    "\n",
    "- If the data was increased by 100x, I would implement the following items:\n",
    "    - Build out specifc key columns across the fact and dimensional tables so that better indexing can be created, making querying the data more efficient.\n",
    "    - Utilize Hadoop to create a distributed computing data architecture for faster processing of big data.\n",
    "    \n",
    "- To update on a daily basis I would implement the following options:\n",
    "    - Create an Data ETL Orchestration environment that can schedule the data pipeline everyday before 7AM. An example product would be Apache Airflow which is a popular orchestration tool used across the data industry\n",
    "    - The need for daily data would also require a more scalable data solution. Utilizing AWS S3 buckets to stage and save these data files would be a reliable solution\n",
    "    \n",
    "- To make the data accessible to over 100+ people I would implent the following:\n",
    "    - Create a customer facing front end environment that could allow customers to read from these tables. Viable options include creating a data virtualization layer such as Cisco Data VIrtualization, and allow customers to subscribe to access to certain schemas.\n",
    "    - Similarly, utlizing OLAP cube services such as Azure Analysis Services is a great option, with public cloud resources able to expand resources as needed when load traffic increases.\n",
    "    - Another option is to create an API layer which customers can utilize to access the data via code, which they could then ingest for their own data needs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
